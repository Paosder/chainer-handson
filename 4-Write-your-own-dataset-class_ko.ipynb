{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--# How to write a custom dataset class-->\n",
    "# 커스텀 데이터셋 클래스를 만드는 방법\n",
    "\n",
    "<!--A typical statagy to improve generalization in deep neural networks is to increase the number of training examples which allows more parameters to be used in the model. Even if the number of parameters is kept unchanged, increasing the number of training examples often improves generalization performance.-->\n",
    "deep neural network에서 일반화 성능을 높이는 전형적인 방법은 학습 데이터의 수를 늘려서 model로 하여금 파라미터를 더 잘 수정하게끔 하는 것입니다. 파라미터의 수가 바뀌지않고도 학습 데이터의 수를 늘리는 것만으로도 일반화 성능을 높여주곤 하거든요.\n",
    "\n",
    "<!--Since it can be tedious and expensive to manually obtain and label additional training examples, a useful stratagy is to consider methods for automatically increasing the size of a training set. Fortunately, for image datasets, there are several augmentation methods that have been found to work well in practice. They include:-->\n",
    "추가적인 학습 데이터를 만들어주는것은 수동으로 하기엔 귀찮고 번거로운 일이 아닐 수 없습니다. 따라서 가장 현명한 방법은 자동으로 학습 데이터셋을 늘려주는 방법을 구상하는 것이겠죠. 다행히도 이미지 데이터셋에는 여러 가지 증강 방법이 제시되어 있기 때문에 실제로 사용해볼 때 유용합니다. 아래와 같은 방법이 포함되어있습니다 :\n",
    "\n",
    "<!--* Randomly cropping several slightly smaller images from the original training image.-->\n",
    "* 원본 학습 이미지에서 임의의 지점을 잘라서 작은 이미지를 만듭니다.\n",
    "<!--* Horizontally flipping the image.-->\n",
    "* 수평을 기준으로 이미지를 뒤집습니다.\n",
    "<!--* Randomly rotating the image.-->\n",
    "* 임의의 각도로 이미지를 돌립니다.\n",
    "<!--* Applying various distortions and or noise to the images, etc.-->\n",
    "* 다양한 왜곡 혹은 잡음을 넣어 이미지를 변형합니다. 기타등등..\n",
    "\n",
    "<!--In this example, we will write a custom dataset class that performs the first two of these augmentation methods on the CIFAR10 dataset. We will then train our previous deep CNN and check that the generalization performance on the test set has in fact improved.-->\n",
    "이 예제에서는 CIFAR10 데이터셋에서 2가지의 증강 방법을 사용하여 커스텀 데이터셋을 만들어 볼 것입니다. 이후 이것을 가지고 학습한 model과 이전의 CNN model을 비교하여 성능이 개선이 되었는지 직접 확인해 볼 것입니다.\n",
    "\n",
    "<!--We will create this dataset augmentation class as a subclass of `DatasetMixin`, which has the following API:-->\n",
    "`DatasetMixin` 이라는 데이터셋 증강 클래스의 서브클래스로써 만들어봅시다. 이 클래스는 아래와 같은 인터페이스를 갖습니다 :\n",
    "\n",
    "<!--- `__len\\__` method to return the size of data in dataset. -->\n",
    "- `__len__` 메서드는 데이터셋의 크기를 반환합니다.\n",
    "<!--- `get_example` method to return data or a tuple of data and label, which are passed by 'i' argement variable. -->\n",
    "- `get_example` 메서드는 i 인수에 의해 데이터 튜플과 라벨을 반환합니다.\n",
    "\n",
    "<!--Other necesary features for a dataset can be prepared by inheriting 'chainer.datase.DasetMixin' class. -->\n",
    "데이터셋의 다른 필수요소들은 'chainer.dataset.DatasetMixin' 클래스를 상속받는 것 만으로도 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## 1. Write the dataset augmentation class for CIFAR10-->\n",
    "## 1. CIFAR10을 기반으로 한 증강 데이터셋 클래스 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import dataset\n",
    "from chainer.datasets import cifar\n",
    "\n",
    "class CIFAR10Augmented(dataset.DatasetMixin):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        train_data, test_data = cifar.get_cifar10()\n",
    "        if train:\n",
    "            self.data = train_data\n",
    "        else:\n",
    "            self.data = test_data\n",
    "        self.train = train\n",
    "        self.random_crop = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        x, t = self.data[i]\n",
    "        if self.train:\n",
    "            x = x.transpose(1, 2, 0)\n",
    "            h, w, _ = x.shape\n",
    "            x_offset = np.random.randint(self.random_crop)\n",
    "            y_offset = np.random.randint(self.random_crop)\n",
    "            x = x[y_offset:y_offset + h - self.random_crop,\n",
    "                  x_offset:x_offset + w - self.random_crop]\n",
    "            if np.random.rand() > 0.5:\n",
    "                x = np.fliplr(x)\n",
    "            x = x.transpose(2, 0, 1)\n",
    "        return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This class performs the following types of data augmentation on the CIFAR10 example images:-->\n",
    "이 클래스는 아래와 같은 데이터 증강 방법을 통해 CIFAR10 데이터셋의 데이터를 증가시키고 있습니다 :\n",
    "\n",
    "<!--- Randomly crop a 28X28 area form the 32X32 whole image data.-->\n",
    "- 32x32 공간에서 임의로 28x28 공간을 자릅니다.\n",
    "<!--- Randomly perform a horizontal flip with 0.5 probability. -->\n",
    "- 매번 확률을 계산하여 50% 이상일 경우 수평방향으로 이미지를 뒤집습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## 2. Train on the CIFAR10 dataset using our dataset augmentation class-->\n",
    "## 2. 커스텀 데이터셋을 통해 학습하기\n",
    "<!--Let's now train the same deep CNN from the previous example. The only difference is that we will now use our dataset augmentation class. Since we reuse the same model with the same number of parameters, we can observe how much the augmentation improves the test set generalization performance.-->\n",
    "이제 이전에 작성했던 깊은 DNN을 학습시켜봅시다. 이전 예제와 다른 점이라 한다면 여기에서는 데이터셋 증강 클래스를 사용해서 학습을 시킬 것이라는 것이죠. 이전 model의 파라미터 또한 그대로 사용하지만 일반화 성능이 증강 데이터셋에서 얼마나 증가하는지 확인해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   main/accuracy  validation/main/loss  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           2.05572     0.2291         1.74695               0.340665                  17.4928       \n",
      "\u001b[J2           1.5648      0.404109       1.34217               0.493531                  34.2186       \n",
      "\u001b[J3           1.30648     0.510563       1.18946               0.54787                   51.0198       \n",
      "\u001b[J4           1.13766     0.59387        0.990061              0.663117                  67.7972       \n",
      "\u001b[J5           0.978814    0.671016       0.833926              0.721835                  84.5843       \n",
      "\u001b[J6           0.869395    0.713128       0.748523              0.752389                  101.349       \n",
      "\u001b[J7           0.762867    0.748119       0.707316              0.770104                  118.147       \n",
      "\u001b[J8           0.706639    0.771907       0.897042              0.761047                  134.912       \n",
      "\u001b[J9           0.648668    0.79186        0.573436              0.811107                  151.68        \n",
      "\u001b[J10          0.596508    0.805738       0.568505              0.810709                  168.436       \n",
      "\u001b[J11          0.555778    0.818902       0.553908              0.82285                   185.246       \n",
      "\u001b[J12          0.528083    0.828565       0.480858              0.842058                  201.999       \n",
      "\u001b[J13          0.495035    0.839034       0.482114              0.845641                  218.756       \n",
      "\u001b[J14          0.465551    0.846971       0.432789              0.853603                  235.496       \n",
      "\u001b[J15          0.441277    0.857714       0.408645              0.86455                   252.341       \n",
      "\u001b[J16          0.419518    0.862536       0.414598              0.863256                  269.1         \n",
      "\u001b[J17          0.407975    0.867527       0.382838              0.87281                   285.88        \n",
      "\u001b[J18          0.387507    0.87438        0.377102              0.874502                  302.7         \n",
      "\u001b[J19          0.37077     0.879741       0.40403               0.872213                  319.468       \n",
      "\u001b[J20          0.355528    0.885363       0.376208              0.881867                  336.24        \n",
      "\u001b[J21          0.338302    0.889886       0.408866              0.875697                  353.08        \n",
      "\u001b[J22          0.329845    0.892806       0.41183               0.869128                  369.837       \n",
      "\u001b[J23          0.316535    0.895487       0.394337              0.879479                  386.547       \n",
      "\u001b[J24          0.300875    0.901588       0.355019              0.886047                  403.367       \n",
      "\u001b[J25          0.299977    0.902813       0.353024              0.884753                  420.126       \n",
      "\u001b[J26          0.285723    0.90569        0.365838              0.886843                  436.868       \n",
      "\u001b[J27          0.277759    0.909651       0.374885              0.886146                  453.673       \n",
      "\u001b[J28          0.269112    0.913672       0.377726              0.88953                   470.425       \n",
      "\u001b[J29          0.263397    0.915161       0.358239              0.892018                  487.178       \n",
      "\u001b[J30          0.255717    0.918054       0.371856              0.892914                  503.978       \n",
      "\u001b[J31          0.249953    0.918254       0.400021              0.88953                   520.718       \n",
      "\u001b[J32          0.247197    0.919794       0.40666               0.886843                  537.476       \n",
      "\u001b[J33          0.239707    0.922175       0.370864              0.894407                  554.306       \n",
      "\u001b[J34          0.22484     0.926016       0.379236              0.891421                  571.064       \n",
      "\u001b[J35          0.223724    0.928737       0.388511              0.891819                  587.84        \n",
      "\u001b[J36          0.21589     0.929557       0.387719              0.888435                  604.652       \n",
      "\u001b[J37          0.212748    0.931506       0.350514              0.901373                  621.418       \n",
      "\u001b[J38          0.208369    0.931438       0.380141              0.901572                  638.169       \n",
      "\u001b[J39          0.203677    0.933159       0.361013              0.901572                  654.96        \n",
      "\u001b[J40          0.204872    0.933599       0.406545              0.893312                  671.705       \n",
      "\u001b[J41          0.191173    0.93778        0.426746              0.894606                  688.482       \n",
      "\u001b[J42          0.187821    0.940741       0.374462              0.900279                  705.295       \n",
      "\u001b[J43          0.182078    0.940901       0.344274              0.901274                  722.037       \n",
      "\u001b[J44          0.181825    0.940801       0.405228              0.895601                  738.755       \n",
      "\u001b[J45          0.178642    0.942695       0.410054              0.895303                  755.508       \n",
      "\u001b[J46          0.177163    0.942542       0.395682              0.897293                  772.208       \n",
      "\u001b[J47          0.165153    0.947003       0.418487              0.899781                  789.001       \n",
      "\u001b[J48          0.16477     0.947623       0.381462              0.902767                  805.733       \n",
      "\u001b[J49          0.161042    0.948409       0.378381              0.900279                  822.472       \n",
      "\u001b[J50          0.161297    0.948363       0.353441              0.906449                  839.239       \n",
      "\u001b[J51          0.159813    0.948544       0.439083              0.903762                  855.978       \n",
      "\u001b[J52          0.156535    0.949464       0.402446              0.898587                  872.714       \n",
      "\u001b[J53          0.158501    0.950088       0.342748              0.904658                  889.507       \n",
      "\u001b[J54          0.150002    0.952125       0.395251              0.90406                   906.208       \n",
      "\u001b[J55          0.148978    0.951865       0.435832              0.90217                   922.942       \n",
      "\u001b[J56          0.143538    0.953405       0.431617              0.90217                   939.702       \n",
      "\u001b[J57          0.140894    0.956282       0.407161              0.901174                  956.448       \n",
      "\u001b[J58          0.140607    0.954585       0.434359              0.902866                  973.167       \n",
      "\u001b[J59          0.141515    0.955726       0.402263              0.908539                  989.945       \n",
      "\u001b[J60          0.137959    0.955926       0.394812              0.907046                  1006.67       \n",
      "\u001b[J61          0.133298    0.957041       0.405913              0.905852                  1023.41       \n",
      "\u001b[J62          0.130892    0.957226       0.442857              0.901174                  1040.16       \n",
      "\u001b[J63          0.133794    0.957426       0.427962              0.907942                  1056.89       \n",
      "\u001b[J64          0.130151    0.958727       0.480573              0.906051                  1073.61       \n",
      "\u001b[J65          0.124034    0.960538       0.44508               0.905752                  1090.4        \n",
      "\u001b[J66          0.125509    0.960267       0.441477              0.903662                  1107.13       \n",
      "\u001b[J67          0.122391    0.961628       0.429096              0.903861                  1123.86       \n",
      "\u001b[J68          0.119709    0.961448       0.419293              0.912619                  1140.63       \n",
      "\u001b[J69          0.12562     0.959938       0.456638              0.903065                  1157.41       \n",
      "\u001b[J70          0.120771    0.962148       0.426555              0.910231                  1174.15       \n",
      "\u001b[J71          0.120427    0.962968       0.451194              0.900179                  1190.94       \n",
      "\u001b[J72          0.113374    0.964509       0.531981              0.901971                  1207.67       \n",
      "\u001b[J73          0.137584    0.959099       0.536572              0.878185                  1224.42       \n",
      "\u001b[J74          0.129289    0.958707       0.485583              0.907345                  1241.18       \n",
      "\u001b[J75          0.104397    0.966309       0.451012              0.905255                  1257.91       \n",
      "\u001b[J76          0.11082     0.965449       0.548796              0.907842                  1274.65       \n",
      "\u001b[J77          0.106523    0.966013       0.549658              0.904658                  1291.45       \n",
      "\u001b[J78          0.107278    0.966029       0.564716              0.905056                  1308.19       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J79          0.100458    0.967229       0.495916              0.909833                  1324.93       \n",
      "\u001b[J80          0.103642    0.966989       0.527921              0.90824                   1341.72       \n",
      "\u001b[J81          0.107007    0.965633       0.507009              0.908041                  1358.47       \n",
      "\u001b[J82          0.102158    0.96833        0.551649              0.903762                  1375.22       \n",
      "\u001b[J83          0.104669    0.967049       0.592808              0.905255                  1391.99       \n",
      "\u001b[J84          0.106911    0.967009       0.546619              0.899781                  1408.7        \n",
      "\u001b[J85          0.10126     0.96853        0.526428              0.909136                  1425.43       \n",
      "\u001b[J86          0.104149    0.96741        0.46494               0.910529                  1442.14       \n",
      "\u001b[J87          0.0992295   0.96903        0.435181              0.907444                  1458.85       \n",
      "\u001b[J88          0.0950871   0.970591       0.516829              0.905653                  1475.63       \n",
      "\u001b[J89          0.0948561   0.970348       0.453227              0.908539                  1492.39       \n",
      "\u001b[J90          0.0953356   0.97029        0.455587              0.907942                  1509.11       \n",
      "\u001b[J91          0.0906988   0.971431       0.530844              0.910629                  1525.83       \n",
      "\u001b[J92          0.0896725   0.971971       0.519109              0.913416                  1542.56       \n",
      "\u001b[J93          0.092792    0.971328       0.544572              0.90834                   1559.35       \n",
      "\u001b[J94          0.0921399   0.972211       0.515931              0.905255                  1576.08       \n",
      "\u001b[J95          0.0912286   0.971631       0.484228              0.910729                  1592.8        \n",
      "\u001b[J96          0.0905843   0.971631       0.488872              0.90615                   1609.5        \n",
      "\u001b[J97          0.0882833   0.972986       0.54652               0.904558                  1626.3        \n",
      "\u001b[J98          0.170932    0.950804       0.497169              0.908937                  1643.04       \n",
      "\u001b[J99          0.0878381   0.972451       0.532973              0.912321                  1659.75       \n",
      "\u001b[J100         0.0830462   0.973532       0.476952              0.914908                  1676.52       \n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import cifar\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "'''\n",
    "현재 이 코드는 GPU를 사용하며 기본 gpu id를 0으로 설정해두고 학습을 수행하고 있습니다.\n",
    "GPU를 이용한 학습을 하지 않으시려면 아래 train 함수에서 표기된 부분을 변경하세요.\n",
    "(model, updater, evaluator)\n",
    "'''\n",
    "class ConvBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_ch, pool_drop=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        '''\n",
    "        # Chainer v1\n",
    "        super(ConvBlock, self).__init__(\n",
    "            conv=L.Convolution2D(None, n_ch, 3, 1, 1,\n",
    "                                 nobias=True, initialW=w),\n",
    "            bn=L.BatchNormalization(n_ch)\n",
    "        )\n",
    "        \n",
    "        self.train = True\n",
    "        self.pool_drop = pool_drop\n",
    "        '''\n",
    "        # Chainer v2\n",
    "        super(ConvBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv = L.Convolution2D(None, n_ch, 3, 1, 1,\n",
    "                                       nobias=True, initialW=w)\n",
    "            self.bn = L.BatchNormalization(n_ch)\n",
    "            self.pool_drop = pool_drop\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn(self.conv(x)))\n",
    "        if self.pool_drop:\n",
    "            h = F.max_pooling_2d(h, 2, 2)\n",
    "            h = F.dropout(h, ratio=0.25)\n",
    "        return h\n",
    "    \n",
    "class LinearBlock(chainer.Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        '''\n",
    "        # Chainer v1\n",
    "        super(LinearBlock, self).__init__(\n",
    "            fc=L.Linear(None, 1024, initialW=w))\n",
    "        self.train = True\n",
    "        '''\n",
    "        # Chainer v2\n",
    "        super(LinearBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.fc = L.Linear(None, 1024, initialW=w)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return F.dropout(F.relu(self.fc(x)), ratio=0.5)\n",
    "    \n",
    "class DeepCNN(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        super(DeepCNN, self).__init__(\n",
    "            ConvBlock(64),\n",
    "            ConvBlock(64, True),\n",
    "            ConvBlock(128),\n",
    "            ConvBlock(128, True),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256, True),\n",
    "            LinearBlock(),\n",
    "            LinearBlock(),\n",
    "            L.Linear(None, n_output)\n",
    "        )\n",
    "        self._train = True\n",
    "            \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "            \n",
    "    @train.setter\n",
    "    def train(self, val):\n",
    "        self._train = val\n",
    "        for c in self.children():\n",
    "            c.train = val\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n",
    "def train(model_object, batchsize=64, gpu_id=0, max_epoch=20):\n",
    "\n",
    "    # 1. Dataset\n",
    "    train, test = CIFAR10Augmented(), CIFAR10Augmented(False)\n",
    "\n",
    "    # 2. Iterator\n",
    "    train_iter = iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = iterators.SerialIterator(test, batchsize, False, False)\n",
    "\n",
    "    # 3. Model\n",
    "    model = L.Classifier(model_object)\n",
    "    model.to_gpu(gpu_id) # GPU 사용\n",
    "\n",
    "    # 4. Optimizer\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # 5. Updater\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id) # GPU 사용\n",
    "\n",
    "    # 6. Trainer\n",
    "    trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='{}_cifar10augmented_result'.format(model_object.__class__.__name__))\n",
    "\n",
    "    # 7. Evaluator\n",
    "\n",
    "    class TestModeEvaluator(extensions.Evaluator):\n",
    "\n",
    "        def evaluate(self):\n",
    "            model = self.get_target('main')\n",
    "            model.train = False\n",
    "            ret = super(TestModeEvaluator, self).evaluate()\n",
    "            model.train = True\n",
    "            return ret\n",
    "\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(TestModeEvaluator(test_iter, model, device=gpu_id)) # GPU 사용\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "    # trainer.extend(extensions.ProgressBar(None, 1000))\n",
    "    trainer.run()\n",
    "    del trainer\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = train(DeepCNN(10), max_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화 정확도가 개선된 것이 보이시나요? 이전 model과 비교해봅시다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with Chainer 2.0.0",
   "language": "python",
   "name": "python3-chainer2.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
